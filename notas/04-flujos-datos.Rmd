# Procesamiento de flujos

En esta parte supondremos que los datos se pueden representar como un flujo de tal velocidad y volumen que típicamente no es posible almacenar todo el flujo, o que no es posible hacer queries a la base de datos resultante.

Veremos técnicas simples para obtener resúmenes simples de los flujos, y también veremos cómo aplicar métodos probabilísticos para filtrar o resumir ciertos aspectos de estos flujos.

Ejemplos de flujos son datos de sitios de internet, datos de redes grandes de sensores, datos de transacciones.

## Muestreo y restricción temporal

Para analizar flujos con estas propiedades podemos hacer:

- Restricción temporal: considerar ventanas de tiempo, y hacer análisis sobre los últimos datos en la ventana. Datos nuevos van reemplazando a datos anteriores, y puede ser que los datos anteriores no se respaldan (o es costoso acceder a ellos).

- Resúmenes acumulados: guardamos resúmenes de los datos que podemos actualizar y utilizar para calcular características de interés en el sistema, por ejemplo conteos simples, promedios.

- Muestreo y filtrado: filtros y muestreo probabilístico. Podemos diseñar muestras apropiadas para estimar cantidades que nos interesen, y sólo guardar los datos que corresponden a la muestra.

Nos interesan principalmente las técnicas de este último punto. Consideraremos dos problemas:

- *Filtrar un flujo*: En general, ¿cómo retener para análisis elementos del flujo que satisfagan una propiedad?
- *Muestreo de un flujo*: ¿Cómo diseñar un esquema de muestreo apropiado?
- *Ventanas resumen*

## Selección de muestra y funciones hash.

Dependiendo de las unidades de muestreo apropiadas que nos
interesen (por ejemplo, clientes o usuarios, transacciones, etc.)
podemos diseñar distintas estrateginas

### Ejemplo {-}
Si queremos estimar el promedio del tamaño de las transacciones en una ventana de tiempo dada, podemos muestrar esa ventana. Cada vez que llega una transacción, usamos un número aleatorio para decidir si
lo incluimos en la muestra o no, y luego hacer nuestro análisis
con las unidades seleccionadas.

```{r, }
install.packages('itertools')
```

```{r, message = FALSE, warning = FALSE}
library(iterators)
library(itertools)
library(tidyverse)
```

```{r}
generar_trans <- function(){
  id_num <- sample.int(500, 1)
  monto <- runif(1, 100,10000) + id_num*10
  trans <- list(id = id_num, monto = monto)
}
```

Podemos estimar la mediana con toda la muestra. Primero generamos
durante 2 segundos, y calculamos la mediana de las transacciones:

```{r}
set.seed(32)
a <- timeout(generar_trans, 2) %>% as.list
length(a)
sapply(a, function(elem) elem$monto) %>% median
```


Si queremos seleccionar un 1\% de las transacciones, 


```{r}
seleccionar_rng <- function(trans){
   runif(1) < 0.01
}
trans_filtradas <- iter(generar_trans, seleccionar_rng)
b <- timeout(trans_filtradas, 2) %>% as.list
length(b)
sapply(b, function(elem) elem$monto) %>% median
```

---



### Ejemplo {-}
Ahora supongamos que queremos estimar el promedio (por cliente) de la transacción máxima de los clientes en una ventana de tiempo. En este caso, la unidad de muestreo más simple es el cliente, y el método del ejemplo anterior es menos apropiado. Quisiéramos en lugar de eso tomar una muestra de
clientes en la ventana, tomar el máximo de todas sus transacciones,
y luego promediar. 

- En este caso, el análisis es más complicado si seleccionamos cada transacción según un número aleatorio (tenderíamos a seleccionar más clientes con muchas transacciones).

```{block2, type ='resumen'}
Podemos usar una función hash del **identificador único de cliente**, y mapear a un cierto número de cubetas $1,\to B$.
Los clientes de la meustra son los que caen en una cubeta (por ejemplo la cubeta 1), y así 
obtendríamos una muestra que consiste de $1/B$ de los clientes
totales que tuvieron actividad en la ventana de interés-
```

- Todos los clientes que tuvieron actividad en la ventana tienen la misma probabilidad de ser seleccionados.
- No es necesario buscar en una lista si el cliente está en la muestra seleccionada o no (lo cual puede ser tardado, o puede ser que terminemos con muestras muy grandes o chicas).
- Este método incorpora progresivamente nuevos clientes a la lista muestreada. Por ejemplo, si la cantidad de clientes está creciendo,
entonces el número de clientes muestreados crecerá de manera correspondiente.



```{r}
a <- timeout(generar_trans, 2) %>% as.list
length(a)
df <- bind_rows(a)
df %>% group_by(id) %>% summarise(monto_max = max(monto)) %>%
  pull(monto_max) %>% median
```

```{r}
seleccionar <- function(trans){
  (21*trans$id %% 23 %% 10) == 0
}
trans_filtradas_cliente <- iter(generar_trans, seleccionar)
b <- timeout(trans_filtradas_cliente, 2) %>% as.list
length(b)
df <- bind_rows(b)
df %>% group_by(id) %>% summarise(monto_max = max(monto)) %>%
  pull(monto_max) %>% median
```


Sin emgargo, esto no funciona con el método de seleccion de arriba:

```{r}
trans_filtradas_cliente <- iter(generar_trans, seleccionar_rng)
b <- timeout(trans_filtradas_cliente, 2) %>% as.list
length(b)
df <- bind_rows(b)
df %>% group_by(id) %>% summarise(monto_max = max(monto)) %>%
  pull(monto_max) %>% median
```

**Observación**: en este último ejemplo, para cada usuario sólo
muestreamos una fracción de sus transacciones. En algunos casos, 
no muestreamos el máximo, y esto produce que la estimación 
esté sesgada hacia abajo.



## Selección de muestra bajo identificadores fijos.

En algunos casos, podemos tener un conjunto $S$ de unidades
que seleccionamos con anterioridad, y quisiéramos seleccionar
en la muestra los datos relacionados con elementos en ese conjunto.

Por ejemplo, quizá nos interesaría muestrar las transacciones
que se hacen en comercios donde han existido fraudes anterioremente,
o quizá checar si un correo proviene de una dirección que está 
en una whitelist (o blacklist)

En estos ejemplo, 
tenemos que checar contra la lista $S$ si seleccionamos un elemento o no, a
diferencia de los ejemplos de arriba.
Cuando la lista es muy grande, esta operación puede ser costosa. Hay
varias opciones para aproximarnos a este problema, aquí veremos
un método probabilístico (Bloom filters) que tiene ventajas en cuanto
a memoria usada.

### Ejemplos {-}

- [Este es un ejemplo](https://gallery.shinyapps.io/087-crandash/) de una
aplicación que cuenta el número de usuarios únicos que bajan paquetes de CRAN. Cada
vez que hay una nueva descarga (transacción) debemos decidir si se trata
de usuarios nuevos o no y actualizar correctamente el conteo de usuarios únicos.

- En este ejemplo de [Medium](https://blog.medium.com/what-are-bloom-filters-1ec2a50c68ff) se usan filtros de Bloom para evitar volver a recomendar artículos ya
vistos o recomendados.

- Supongamos que tenemos un diccionario de palabras $S$ del español.
Cuando observamos una nueva "palabra" que alguien escribió,
queremos saber si la palabra está en el diccionario. Por ejemplo,
para decidir si es un posible error de ortografía o proponer algún sustituto.

- Decidir si una dirección web está en una lista negra, para dar una advertencia
inmediata (*safe browsing*).

---

Una solución a este problema es el filtro de bloom, que es
un esquema probabilístico para filtrar elementos de un flujo
que pertenecen a una colección fija $S$.

## Filtro de Bloom

Consideremos entonces el problema de filtrar de un flujo solamente los elementos que pertenezcan a un conjunto S.

Un filtro de Bloom consiste de:

- Un conjunto $\Omega$ de posibles valores (el universo) que puede aprecer en el flujo
- Un subconjunto $S\subset \Omega$
 de valores que están en la muestra.
- Un vector $v$ de $n$ bits, originalmente igual a 0.
- Una colección de funciones hash $h_1,h_2,\ldots, h_k$ escogidos al azar,
que mapean elementos de $\Omega$ a $n$ cubetas.
 
Queremos decidir si un elemento nuevo $\omega\in \Omega$ está o no en el
conjunto $S.


### Ejemplo {-}


#### Paso 1: inicialización y selección de hashes {-}

Usaremos un vector de tamaño $n=11$ (longitud de vector de bits), y suponemos
que los valores posibles ($\Omega$) son los enteros de uno a mil. Queremos
detectar cuando observamos algún elemento de $S=\{15,523,922\}$. Para este
ejemplo usamos $k=2$ funciones hash. Estas funciones deben mapear
los enteros del uno al mil a las cubetas 1 a 11 (el número de entradas del
 vector e bits). 

```{r}
S <- c(15, 523, 922)
hash_lista <- list(h_1 = function(x) x %% 11 + 1,
                   h_2 = function(x) (5*x + 3) %% 11 + 1)
```

Inicializamos el vector de bits:

```{r}
v <- rep(0, 11)
```


#### Paso 2: insertar elementos en filtro {-}

```{r}
for(i in 1:length(S)){
  indices <- sapply(hash_lista, function(h) h(S[i]))
  indices
  print(indices)
  v[indices] <- 1 
  print(v)
}
```

Y tenemos el vector del filtro listo:
```{r}
v
```

#### Paso 3: filtrar elementos {-}

Ahora veamos cómo decidimos cuáles elementos están o no en el conjunto $S$. Si
observamos un nuevo número $x$, calculamos sus hashes, y vemos si esas
posiciones están prendidas en el vector $v$. Si no lo están, entonces el
elemento necesariamente no está en el conjunto $S$:

```{r}
x <- 219
h_x <- sapply(hash_lista, function(h) h(x))
h_x
```

No está en la lista, pues por lo menos uno de los bits es igual a cero:

```{r, warning=FALSE}
v[h_x]
all(v[h_x])
```

Nunca descartamos un número en la colección:

```{r, warning=FALSE}
x <- 523
h_x <- sapply(hash_lista, function(h) h(x))
all(v[h_x])
```

Sin embargo, puede haber falsos positivos:

```{r, warning=FALSE}
x <- 413
h_x <- sapply(hash_lista, function(h) h(x))
all(v[h_x])
```

**Observación**: nótese que solo es neceario almacenar el vector de bits
y las funciones hash, y esto generalmente resulta en una representación 
compacta. Por otra parte, tendremos algunos falsos positivos, que tenemos
que controlar.

## Análisis de filtro de Bloom

Para construir este filtro, tenemos que escoger el tamaño del vector de bits ($n$),
y el número de funciones hash $k$, dependiendo
del número de elementos que tenemos que almacenar.

Supongamos como aproximación que una función hash en particular selecciona
una de las entradas del bit la misma probabilidad. La probabilidad de que un
bit dado no se encienda cuando insertamos un elemento es
$$1-\frac{1}{n}$$.
Si $k$ es el número de funciones hash, entonces la probabilidad de que ese bit
dado no se encienda es
$$\left (1-\frac{1}{n}\right )^k$$.
Si insertamos $s$ elementos de $S$, entonces la probabilidad de que ese bit dado
no se encienda es entonces
$$\left (1-\frac{1}{n}\right  )^{ks}$$.
La probabilidad de que se encienda es
$$1-\left (1-\frac{1}{n}\right )^{ks}$$.


Finalmente podemos calcular la probabilidad de un falso positivo. Para un elemento
que no está en $S$, la probabilidad de que todos sus hashes caigan en bits encendidos
es

$$ \left ( 1-\left (1-\frac{1}{n}\right )^{ks}\right )^k$$

**Observaciones**:

1. Si usamos un vector más grande ($n$ más grande), la probabildad de falsos
positivos baja (el vector de bits tiene relativamente más ceros).
2. Si el conjunto $S$ es más grandes ($s$ más grande), la probabilidad de
falsos positivos sube (el vector de bits está más lleno).
3. El número de hashes tiene dos efectos: por un lado, más hashes llenan más
el vector de bits de unos. Por otro lado, es más difícil que un nuevo elemento
"atine" a más posiciones que tienen un bit encendido.
4. Esta fórmula es una aproximación, pues usamos funciones hash y no aleatorización.


Podemos hacer una gráfica para ver cómo se comporta la tasa de falsos positivos:


```{r, fig.width = 8}
tasa_fp <- function(n, s, k) {
    (1 - (1 - (1 / n)) ^ (k * s)) ^ k
}
df <- expand.grid(list(s = c(1e5, 1e6, 1e7, 1e8),
                  k = seq(1, 20),
                  n = 2^seq(20,30,1)
                  )) %>%
      mutate(millones_bits = round(n/1e6)) %>%
      mutate(tasa_falsos_p = tasa_fp(n, s, k)) %>%
      mutate(s_str = paste0(s, ' insertados'))


ggplot(df, aes(x = k, y = tasa_falsos_p, 
               colour=(millones_bits), group=millones_bits)) + 
               geom_line(size=1.2) +
               facet_wrap(~s_str) +
               labs(x="k = número de hashes", 
                    colour = "Mill bits \n en vector") +
               scale_y_sqrt()
  
```


Haciendo algunas aproximaciones, se puede demostrar que el número de hashes
óptimo es aproximadamente
$$k  = \frac{n}{s}\log(2)$$

```{r}
df_opt <- df %>% select(n, s) %>%  
  mutate(k = ceiling((n/s)*log(2))) %>% unique %>%
  mutate(tasa_falsos_p = tasa_fp(n, s, k)) %>%
  mutate(s_str = paste0(s, ' insertados'))
ggplot(df, aes(x = k, y = tasa_falsos_p)) +
               geom_line(aes(colour=(millones_bits), group=millones_bits),
                 size=1.2) +
               facet_wrap(~s_str) +
               labs(x="k = número de hashes", 
                    colour = "Mill bits \n en vector") +
               scale_y_sqrt() +
               geom_point(data = df_opt, col='red') +
               xlim(0,20)
  
```


## Ejemplo: un corrector de ortografía basado en filtro de Bloom

```{r}
diccionario <- read.csv("../datos/diccionario/es_dic.txt", header = FALSE, stringsAsFactors =FALSE)
diccionario <- iconv(diccionario[, 1], to = 'utf-8')
m <- length(diccionario)
m
```

Queremos insertar entonces unos 250 mil elementos, aunque puede ser posible
que quizá queramos insertar otras palabras más adelante.

```{r}
df <- expand.grid(list(s = 300000,
                  k = seq(4, 10),
                  n = c(1e6, 2e6, 4e6, 8e6)
                  )) %>%
      mutate(millones_bits = (n/1e6)) %>%
      mutate(tasa_falsos_p = tasa_fp(n, s, k)) %>%
      mutate(s_str = paste0(s, ' insertados'))


ggplot(df, aes(x = k, y = tasa_falsos_p, 
               colour=(millones_bits), group=millones_bits)) + 
               geom_line(size=1.2) +
               facet_wrap(~s_str) +
               labs(x="k = número de hashes", 
                    colour = "Mill bits \n en vector") +
               scale_y_log10(breaks= c(0.0001, 0.001, 0.01, 0.1))
```

Pdemos intentar usar un vector de 8 millones de bits con 10 hashes. Nuestra
estimación de falsos positivos es de

```{r}
n <- 8e6
tasa_fp(n, 3e5, 10)
```

Ahora necesitamos nuestras funciones hash escogidas al azar. Podemos
usar el algoritmo murmur32, por ejemplo:

```{r}
library(digest)
set.seed(123)
hash_generator <- function(k = 10, n){
  seeds <- sample.int(652346, k)
  hasher <- function(x){
    sapply(seeds, function(seed){
      sub_str <- substr(digest::digest(x, "xxhash32", seed = seed), 1, 7)
      strtoi(sub_str, base = 16L) %% n + 1
    })
  }
  hasher
}
hashes <- hash_generator(10, n)  
saveRDS(hashes, file = 'salidas/hash_diccionario.rds')
```

```{r}
hashes('él')
hashes('el')
hashes('árbol')
```

Ahora insertamos las palabras del diccionario en el filtro. Esta operación
toma tiempo si queremos insertar muchos elementos, pero solamente
hay que hacerlo una vez (puede paralelizarse):

```{r, eval = FALSE}
v <- rep(FALSE, n)
system.time(
for(i in seq_along(diccionario)){
  hashes_palabra <- hashes(diccionario[i])
  v[hashes_palabra] <- TRUE
}
)
saveRDS(v, file = 'salidas/vector_diccionario.rds')
sum(v)
```

```{r}
v <- readRDS(file = 'salidas/vector_diccionario.rds')
hashes <- readRDS(file = 'salidas/hash_diccionario.rds')
```

Creamos una función para filtrar:

```{r}
pertenece <- function(palabra){
  hashes_palabra <- hashes(iconv(palabra, to = 'utf-8'))
  all(v[hashes_palabra])
}
```


Ahora veamos unos ejemplos

```{r}
pertenece('árbol')
```

```{r}
palabras_prueba <- c('árbol', 'arbol', 'explicásemos', 'xexplicasemos',
                     'gato', 'perror', 'perro', 'alluda','ayuda')
df_palabras <- data_frame(palabra = palabras_prueba) %>%
                   mutate(pertenece = map_lgl(palabra, pertenece))
df_palabras
```

En el siguiente paso tendríamos que producir sugerencias de corrección.
En caso de encontrar una palabra que no está en el diccionario,
podemos producir palabras similares (a cierta distancia de edición),
y filtrar aquellas que pasen el filtro de bloom (ver [How to write a spelling corrector](http://norvig.com/spell-correct.html)).

```{r}
generar_dist_1 <- function(palabra){
  caracteres <- c(letters, 'á', 'é', 'í', 'ó', 'ú', 'ñ')
  pares <- lapply(0:(nchar(palabra)), function(i){
    c(str_sub(palabra, 1, i), str_sub(palabra, i+1, nchar(palabra)))
  })
  eliminaciones <- pares %>% map(function(x){ paste0(x[1], str_sub(x[2],2,-1))})
  sustituciones <- pares %>% map(function(x)
      map(caracteres, function(car){
    paste0(x[1], car, str_sub(x[2], 2 ,-1))
  })) %>% flatten 
  inserciones <- pares %>% map(function(x){
    map(caracteres, function(car) paste0(x[1], car, x[2]))
  }) %>% flatten
  transposiciones <- pares %>% map(function(x){
    paste0(x[1], str_sub(x[2],2,2), str_sub(x[2],1,1), str_sub(x[2],3,-1))
  })
  c(eliminaciones, sustituciones, transposiciones, inserciones)
}
```

```{r}
generar_dist_1('perror') %>% keep(pertenece)
```

```{r}
generar_dist_1('explicasemos') %>% keep(pertenece)
```


```{r}
generar_dist_1('hayuda') %>% keep(pertenece)
```
