# Similitud

En esta parte trata de un problema fundamental en varias tareas de minería de datos: ¿cómo medir similitud, y cómo encontrar vecinos cercanos en un conjunto de elementos?

Algunos ejemplos son:

- Encontrar documentos similares en una colección de documentos (este es el que vamos a tratar más). Esto puede servir para detectar
plagio, deduplicar noticias o páginas web, etc.
- Encontrar imágenes similares en una colección grande.
- Encontrar usuarios similares (Netflix), en el sentido de que tienen gustos similares. O películas similares, en el sentido de qe le gustan a las mismas personas
- Uber: rutas similares que indican (fraude o abusos)[https://eng.uber.com/lsh/].

Estos problemas no son triviales por dos razones:

- Los elementos que queremos comparar muchas veces están naturalmente representados en espacios de dimensión muy alta, y es relativamente costoso comparar un par (documentos, imágenes, usuarios, rutas).
- Si la colección de elementos es grande ($N$), entonces el número de pares 
posibles es del orden de $N^2$, y no es posible hacer todas las posibles comparaciones para encontrar los elementos similares.

El tema principal de esta parte es el siguiente:

```{block, type='resumen'}
- Podemos usar reducción probabilística de dimensión (usando funciones hash) para reducir la dimensionalidad del problema de similitud, sin
perder mucha precisión en el cálculo de similitudes.
- Podemos usar métodos probabilísticos para agrupar elementos similares (encontrar vecinos cercanos), sin necesidad de calcular TODAS las similitudes posibles.
```

## Similitud de conjuntos

Muchos de estos problemas de similitud se pueden pensar como 
problemas de similitud entre conjuntos. Por ejemplo, los documentos son conjuntos de palabras, pares de palabras, sucesiones de caracteres, etc,
una película como el conjunto de personas a las que le gustó, o una ruta
como un conjunto de tramos, etc.

Hay muchas medidas que son útiles para cuantificar la similitud entre conjuntos. Una que es popular, y que explotaremos por sus propiedades, es la similitud de Jaccard:


```{block2, type='resumen'}
La **similitud de Jaccard** de los conjuntos $A$ y $B$ está dada por

$$sim(A,B) = \frac{|A\cap B|}{|A\cup B|}$$

```

Esta medida cuantifica qué tan cerca está la unión de $A$ y $B$ de su intersección. Cuanto más parecidos sean $A\cup B$ y $A\cap B$, más similares son los conjuntos. En términos geométricos, es el área de la intersección entre el área de la unión.

#### Ejercicio {-}
Calcula la similitud de jaccard entre los conjuntos $A=\{5,2,34,1,20,3,4\}$
 y $B=\{19,1,2,5\}$
 

```{r, collapse = TRUE, warning=FALSE, message=FALSE}
library(tidyverse)
library(textreuse)
library(tm)

sim_jaccard <- function(a, b){
    length(intersect(a, b)) / length(union(a, b))
}

sim_jaccard(c(0,1,2,5,8), c(1,2,5,8,9))
sim_jaccard(c(2,3,5,8,10), c(1,8,9,10))
sim_jaccard(c(3,2,5), c(8,9,1,10))
```


## Representación en tejas para documentos

En primer lugar, buscamos representaciones
de documentos como conjuntos. Hay varias maneras de hacer esto. 

Consideremos una colección de textos cortos:

```{r}
textos <- character(4)
textos[1] <- 'el perro persigue al gato.'
textos[2] <- 'el gato persigue al perro'
textos[3] <- 'este es el documento de ejemplo'
textos[4] <- 'el documento con la historia del perro y el gato'
```


- La representación
más simple es la bolsa de palabras, que es conjunto de palabras que contiene un
documento. Podríamos comparar entonces documentos calculando la similitud de jaccard 
de sus bolsas de palabras (1-gramas)

```{r}
tokenize_words(textos[1])
```

- Podemos generalizar esta idea y pensar en n-gramas de palabras, que son sucesiones
de $n$ palabras que ocurren en un documento.

```{r}
tokenize_ngrams(textos[1], n = 2)
```


- Otro camino, que es el que veremos aquí, es el k-tejas, que son k-gramas de *caracteres*

```{r, collapse= TRUE}
library(textreuse)
shingle_chars <- function(string, lowercase = FALSE, k = 3){
    # produce shingles (con repeticiones)
    if(lowercase) {
      string <- str_to_lower(string)
    }
    shingles <- seq(1, nchar(string) - k + 1) %>%
        map_chr(function(x) substr(string, x, x + k))
    shingles
  }
ejemplo <- shingle_chars('Este es un ejemplo', 3)
ejemplo
```

Si lo que nos interesa principalmente
similitud textual (no significado, o polaridad, etc.) entre documentos, entonces podemos comparar dos documentos considerando que sucesiones de caracteres de tamaño fijo ocurren en ambos documentos. Esta
representación es **flexible** en el sentido de que se puede adaptar para documentos muy cortos (mensajes o tweets, por ejemplo), pero también para documentos más grandes.

```{block2, type = 'resumen'}
**Tejas (shingles)**
  
Sea $k>0$ un entero. Las $k$-tejas ($k$-shingles) de un documento d
 es el conjunto de todas las corridas (distintas) de $k$
caracteres sucesivos.

```

Es importante escoger suficientemente grande, de forma que la probabilidad de que
una teja particular tenga probabilidad baja de ocurrir en un texto dado. Si los textos
son cortos, entonces basta tomar valores como $k=4,5$, pues hay un total de $27^4$ tejas
de tamaño 4, y el número de tejas de un documento corto (mensajes, tweets) es mucho más bajo que
$27^4$ (nota: ¿puedes explicar por qué este argumento no es exactamente correcto?)

Para documentos grandes, como noticias o artículos, es mejor escoger un tamaño más grande,
como $k=9,10$, pues en documentos largos puede haber cientos de miles
de caracteres, si $k$ fuera más chica entonces una gran parte de las tejas aparecería en muchos de los documentos.

#### Ejemplo {-}
Documentos textualmente similares tienen tejas similares:

```{r, collapse = TRUE}
textos <- character(4)
textos[1] <- 'el perro persigue al gato, pero no lo alcanza'
textos[2] <- 'el gato persigue al perro, pero no lo alcanza'
textos[3] <- 'este es el documento de ejemplo'
textos[4] <- 'el documento habla de perros, gatos, y otros animales'
tejas_doc <- lapply(textos, shingle_chars, k = 3)
sim_jaccard(tejas_doc[[1]], tejas_doc[[2]])
sim_jaccard(tejas_doc[[1]], tejas_doc[[3]])
sim_jaccard(tejas_doc[[4]], tejas_doc[[3]])
```


## Reducción probablística de dimensión.

La representación de k-tejas de documentos es una representación de dimensión alta (pues
hay muchas tejas), pues cada documento se escribir como un vector de 0s y 1s:

```{r}
todas_tejas <- Reduce('c', tejas_doc) %>% unique %>% sort
vector_1 <- as.numeric(todas_tejas %in% tejas_doc[[1]])
vector_1
```

Para esta colección chica, con $k$ relativamente chico, el vector
que usamos para representar cada documento es de tamaño `r length(vector_1)`,
pero en otros casos este número será mucho más grande. 

Podemos construir expícitamente la matriz de tejas-documentos de las siguiente forma (OJO: esto normalmente **no** queremos hacerlo, pero lo hacemos para ilustrar):


```{r}
df <- data_frame(id_doc = paste0('doc_',
                                 seq(1, length(tejas_doc))),
           tejas = tejas_doc) %>% 
           unnest %>%
           unique %>%
           mutate(val = 1) %>%
           spread(id_doc, val, fill = 0) 
df
```



¿Cómo calculamos la similitud de Jaccard usando estos datos?

Calcular la unión e intersección se puede hacer haciendo OR y AND de las columnas, y
entonces podemos calcular la similitud
```{r}

inter_12 <- sum(df$doc_1 & df$doc_2)
union_12 <- sum(df$doc_1 | df$doc_2)
similitud <- inter_12/union_12
similitud # comparar con el número que obtuvimos arriba.
```


Ahora consideramos una manera probabilística de reducir la
dimensión de esta matriz sin perder información útil para
calcular similitud. Queremos obtener una matriz con menos renglones
(menor dimensión) y las mismas columnas.

Las proyecciones que usaremos son escogidas al azar, y son sobre
el espacio de enteros.

- Sea $\pi$ una permutación al azar de los renglones de la matriz.
- Permutamos los renglones de la matriz tejas-documentos según $\pi$.
- Definimos una nuevo descriptor del documento: para cada documento $d$, tomamos el entero $h_\pi (d)$, que da el número del primer renglón de $d$ que es distinto de 0


#### Ejemplo {-}

Ordenamos al azar:
```{r}
set.seed(321)
df_1 <- df %>% sample_n(nrow(df))
head(df_1, 14)
```



```{r}
mayor_uno <- function(x) x > 0
primer_uno <- function(col){
  detect_index(col, mayor_uno)
}
df_1 %>% summarise_if(is.numeric, primer_uno) 
```

Ahora repetimos con otras permutaciones:

```{r}
set.seed(29)
firmas_df <- lapply(1:5, function(i){
    df_1 <- df %>% sample_n(nrow(df))
    df_1 %>% summarise_if(is.numeric, primer_uno) 
}) %>% bind_rows()

firmas_df <- firmas_df %>% add_column(firma = paste0('h_', 1:5), .before = 1)
firmas_df
```

A esta nueva matriz le llamamos **matriz de firmas** de los documentos.  La firma de un documento es una sucesión de enteros.

Cada documento se describe ahora con `r nrow(firmas_df)` entradas,
en lugar de `r nrow(df_1)`.

Nótese que por construcción, cuando dos documentos son muy similares,
es natural que sus columnas de firmas sean similares, pues al hacer las permutaciones
es altamente probable que el primer 1 ocurra en la misma posición.

Resulta que podemos cuantificar esta probabilidad. Tenemos el siguiente
resultado simple pero sorprendente:

```{block2, type = 'resumen'}
Sea $\pi$ una permutación escogida al azar, y $a$ y $b$ dos columnas
dadas. Entonces
$$P(h_\pi(a) = h_\pi(b)) = sim(a, b)$$
donde $sim$ es la similitud de jaccard basada en las tejas usadas.

Sean $\pi_1, \pi_2, \ldots \pi_n$ permutaciones escogidas al azar de
manera independiente. Si $n$ es grande, entonces por la ley de los grandes números
$$sim(a,b) \approx \frac{|\pi_j : h_{\pi_j}(a) = h_{\pi_j}(b)|}{n}, $$
es decir, la similitud de jaccard es aproximadamente la proporción 
de elementos de las firmas que coinciden.
```


#### Ejemplo {-}
Antes de hacer la demostración, veamos como aplicaríamos a la matriz
de firmas que calculamos arriba. Tendríamos, por ejemplo :
```{r, collapse = TRUE}
mean(firmas_df$doc_1 == firmas_df$doc_2)
mean(firmas_df$doc_1 == firmas_df$doc_3)
mean(firmas_df$doc_3 == firmas_df$doc_4)

```

Ahora veamos qué sucede si usamos más firmas:

```{r, collapse = TRUE}
firmas_df <- lapply(1:40, function(i){
    df_1 <- df %>% sample_n(nrow(df))
    df_1 %>% summarise_if(is.numeric, primer_uno) 
}) %>% bind_rows()

firmas_df <- firmas_df %>% add_column(firma = paste0('h_', 1:40), .before = 1)
firmas_df
mean(firmas_df$doc_1 == firmas_df$doc_2)
mean(firmas_df$doc_1 == firmas_df$doc_3)
mean(firmas_df$doc_3 == firmas_df$doc_4)
```

---

Ahora damos un argumento de este resultado.
Consideremos dos columnas $a,b$ de la matriz
de 0's y 1's, con conjuntos de tejas asociados $A,B$.

- Supongamos que entre las dos columnas $a$ y $b$, el primer 1 ocurre en el renglón $k$.
- El renglón $k$ puede ser de tipo $(1,0), (0,1), (1,1)$. Todos estos renglones tienen la misma probabilidad de aparecer en el rengón k. El número de estos
renglones es el tamaño de $A\cup B$, pues este número cuenta cuántas tejas en común
tienen estos conjuntos.
- El número de renglones de tipo  $(1,1)$ es el tamaño de $A\cap B$, el número de tejas en común de los dos documentos.
- Entonces, la probabilidad condicional de que el renglón $k$ sea de tipo $(1,1)$, dado que es de algún tipo de $(1,0), (0,1), (1,1)$, es 
$$\frac{|A\cap B|}{|A\cup B|},$$
que es la similitud de Jaccard de los dos documentos.


## Min-hashing

En la sección anterior propusimos una manera probabilítica de 
reducir dimensionalidad para el problema de calcular similitud de Jaccard usando proyecciones aleatorias de los datos
basadas en permutaciones de las tejas. Esto nos da una representación
más compacta, que podemos muchas veces almacenar en memoria.

El problema con el procedimiento de arriba es el costo de calcular las permutaciones,
permutar los renglones,
y buscar el primer elemento de cada columna que es distinto de 0. 

Podemos hacer una aproximación adicional: en lugar de calcular permutaciones,

- Escogemos funciones al azar que mapean tejas (cadenas) a enteros de forma determinística.
- Aplicamos una de estas
funciones a cada una de las tejas, y obtenemos una lista de enteros que interpretaremos como el orden de una permutación.

Las funciones usadas deben comportarse de manera aproximadamente aleatoria. 
Tomemos un ejemplo simple

```{r}
crear_hash <- function(m){
  primo <- 773
  a <- sample(1:primo)
  b <- sample(1:primo)
  function(x){ (a*x + b) %% p %% m}
}
```


### Funciones hash







