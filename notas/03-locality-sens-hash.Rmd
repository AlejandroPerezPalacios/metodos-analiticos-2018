# Similitud: Locality sensitive hashing

```{r, echo=FALSE, message=FALSE}
library(tidyverse)
cb_palette <- c("#000000", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
```


En esta parte continuaremos con la búsqueda de pares similares
para colecciones de textos, y después mostraremos cómo aplicar
estas técnicas para otras medidas de distancia  (como distancia
euclideana y coseno). 

Como vimos en la parte anterior, la técnica de LSH (locality sensitive
hashing) consiste en poner en cubetas a elementos que tengan 
hashes similares. Si diseñamos correctamente el método, entonces
no es necesario hacer todas las comparaciones entre los pares,
y basta examinar los elementos que compartan cubeta con otros elementos
(eliminando la mayor parte de las cubetas que tendrán solo un elemento).


## Análisis de la técnica de bandas 

En la sección anterior dimos la primera idea como usar
la *técnica de bandas* con minhashes para encontrar documentos de similitud alta, con distintos umbrales de similitud alta. Aquí describimos un análisis
más detallado de la técncia

```{block2 , type='resumen'}
Supongamos que tenemos un total de $k$ minhashes, que dividimos
en $b$ bandas de tamaño $r$, de modo que $k=br$. 

- Decimos que un par de documentos *coinciden* en una banda de $r$ hashes
si coinciden en todos los hashes de esa banda.

- Un par de documentos es un **par candidato** si 
por al menos coinciden en una banda (es decir, en al menos dentro
de una banda todos los hashes coinciden).

```

Ahora vamos a calcular la probabilidad de que un par de documentos
con similitud $s$ sea un par candidato:

1. La probabilidad de que estos dos documentos coincidan en un hash
particular es $s$, la similitud de Jaccard.
2. La probabiliad de que todos los hashes de una banda coincidan es
$s^r$, pues seleccionamos los hashes independientemente. 
3. Así que la probabilidad de que los documentos no coincidan en una banda
particular es:
es $1-s^r$
4. Esto implica que la probabilidad de que los documentos no coincidan en ninguna banda es $(1-s^r)^b$.
5. Finalmente, la probabilidad de que estos dos documentos sea un par candidato es $1-(1-s^r)^b$, que es la probabilidad de que coincidan en al menos una banda.

```{block2, type='resumen'}
Si la similitud de jaccard de dos documentos es $s$, la probabilidad
de que sean un par candidato es igual a $$1-(1-s^r)^b$$.

```



### Ejemplo {-}
Supongamos que tenemos 8 minhashes, y que nos
interesa encontrar documentos con similitud mayor a 0.7. 
Tenemos las siguientes posiblidades:
```{r, fig.width=4, fig.asp=0.6}
graficar_curvas <- function(df_br, colour = TRUE){
  r <- df_br$r
  b <- df_br$b
  datos_graf <- data_frame(s = seq(0, 1, 0.01))
  curvas_similitud <- data_frame(b = b, r =r) %>%
                    group_by(r, b) %>%
                    mutate(datos = map2(r, b, function(r, b){
                      datos_graf %>% 
                      mutate(prob = 1 - (1 - s ^ r) ^b)
                    })) %>%
                    unnest
  graf_salida <- ggplot(curvas_similitud, 
                        aes(x = s, y = prob, 
                            colour = as.factor(interaction(b,r)))) +
                 geom_line(size=1.1) + 
                 labs(x = 'similitud', y= 'probablidad de ser candidato',
                      colour = 'b.r') 
  if(colour){
    graf_salida + scale_colour_manual(values=cb_palette)
  }
                 
  graf_salida
}
r <- c(1,2,4,8)
df_br <- data_frame(r = r, b = rev(r))
graficar_curvas(df_br) + 
                 geom_vline(xintercept = 0.7)
```

- Con la configuración $r=8, b=1$ (un solo grupo de 8 hashes) es posible
que no capturemos muchos pares de la similitud que nos interesa.
- Con $r=1, b=8$ (al menos un hash de los 8), dejamos pasar 
demasiados falsos positivos, que después vamos a tener que filtrar.
- Los otros dos casos son mejores para nuestro propósito. $r=2$ produce falsos negativos que hay que filtrar, y para $r=4$ hay una probabilidad de alrededor de 50\%
de que no capturemos pares con similitud cercana a 0.7

Generalmente quisiéramos obtener algo más cercano a una función escalón.
Podemos acercarnos si incrementamos el número total de hashes.

```{r, fig.width=4, fig.asp=0.6}
r <- c(4, 5, 8, 10, 20)
b <- 80/r
graficar_curvas(data_frame(b, r)) +
                 geom_vline(xintercept = 0.7) 
```

---

**Observación**: La curva alcanza probabilidad 1/2 cuando la similitud
es
$$s = \left (1-\left (0.5\right )^{1/b} \right )^{1/r}.$$
Y podemos usar esta fórmula para escoger valores de $b$ y $r$ apropiados,
dependiendo de que similitud nos interesa capturar (quizá moviendo un poco
hacia abajo si queremos tener menos falsos negativos).
```{r}
lsh_half <- function(h, b){
   (1 - (0.5) ^ ( 1/b))^(b/h)
}
lsh_half(20,5)
```

En [@mmd], se utiliza la aproximación (de el nivel de similitud
con máxima pendiente de la curva S):
```{r}
textreuse::lsh_threshold
```

```{r}
textreuse::lsh_threshold(20,5)
```
### Ejemplo {-}

Supongamos que nos interesan documentos con similitud mayor a 0.5,
con 50 o 100 hashes:
Algunas combinaciones que podemos tratar son:

```{r, fig.width=5, fig.asp=0.6}
params_umbral <- function(num_hashes, umbral_inf, umbral_sup){
  b <- seq(1, num_hashes)
  b <- b[ num_hashes %% b == 0]
  r <- num_hashes %/% b
  combinaciones_pr <- 
    data_frame(b = b, r = r) %>%
    unique() %>%
    mutate(s = (1 - (0.5)^(1/b))^(1/r)) %>%
    filter(s < umbral_sup, s > umbral_inf)
  combinaciones_pr
}

combinaciones_50 <- params_umbral(50, 0.0, 0.5)
combinaciones_120 <- params_umbral(120, 0.0, 0.5)

graficar_curvas(combinaciones_50)
```

Con 120 hashes podemos obtener curvas con mayor pendiente:

```{r, fig.width=5, fig.asp=0.6}
graficar_curvas(combinaciones_120)
```

**Observación**: La decisión de los valores para estos parámetros
debe balancear qué tan importante es tener pares no detectados,
y el cómputo necesario para calcular los hashes y filtrar los
falsos positivos.


## Resumen de LSH basado en minhashing

Resumen de [@mmd]

1. Escogemos un número $k$ de tamaño de tejas, y construimos el
conjunto de tejas de cada documento.
2. Ordenar los pares documento-teja y agrupar por teja.
3. Escoger $n$, el número de minhashes. Aplicamos el algoritmo de la
clase anterior (teja por teja) para calcular las 
firmas minhash de todos los documentos. 
4. Escoger el umbral $s$ de similitud que nos ineresa. Escogemos $b$ y $r$
(número de bandas y de qué tamaño), usando la fórmula de arriba hasta
obtener un valor cercano al umbral. 
Si es importante evitar falsos negativos, escoger valores de b y r que
den un umbral más bajo, si la velocidad es importante entonces escoger
para un umbral más alto y evitar falsos positivos.
5. Construir pares similares usando LSH
6. Examinar las firmas de cada par candidato y determinar si 
la fracción de coincidencias sobre todos los minhashes es satisfactorio.
Alternativamente (más preciso), calcular directamente la similitud 
de jaccard a partir de las tejas originales. 


Alternativamente,

2a. Agrupar las tejas de cada documento
3b. Escoger $n$, el número de minhashes. Calcular el minhash de cada
documentos aplicando las funciones hash a cada conjunto de tejas y
tomando el mínimo.

## Ejemplo: artículos de wikipedia

En este ejemplo intentamos encontrar artículos similares de [wikipedia](http://wiki.dbpedia.org/datasets/dbpedia-version-2016-10)
 usando las categorías a las que pertenecen. En lugar de usar tejas,
usaremos categorías a las que pertenecen. Dos artículos tienen similitud 
alta cuando los conjuntos de categorías a las que pertenecen es similar.

```{r, engine='bash'}
head -20 ../datos/similitud/wiki-100000.txt
```


Primero hacemos una versión en memoria  usando *textreuse*

```{r}
library(textreuse)
limpiar <- function(lineas,...){
  df_lista <- str_split(lineas, ' ') %>% 
    keep(function(x) x[1] != '#') %>%
    transpose %>%
    map(function(col) as.character(col)) 
  df <- data_frame(articulo = df_lista[[1]], 
                   categorias = df_lista[[2]]) 
  df
}
filtrado <- read_lines_chunked('../datos/similitud/wiki-100000.txt',
                    skip = 1, callback = ListCallback$new(limpiar))
articulos_df <- filtrado %>% bind_rows %>%
                group_by(articulo) %>%
                summarise(categorias = list(categorias))
```

```{r}
articulos_df %>% sample_n(10)
```

### Selección de número de hashes y bandas {-}

Ahora supongamos que buscamos artículos con similitud mínima
de 0.4. Experimentando con valores del total de hashes y el número
de bandas, podemos seleccionar, por ejemplo:

```{r, collapse = TRUE, fig.width=5, fig.asp=0.6}
b <- 20
lsh_half(60, b = b)
graficar_curvas(data_frame(b = b, r = 60/b)) +
                 geom_vline(xintercept = 0.4) 
```



### Tejas y cálculo de minhashes {-}

```{r}
options("mc.cores" = 4L)
tokenize_sp <- function(x) str_split(x, ' ', simplify = TRUE)
shingle_chars <- function(string, lowercase = FALSE, k = 3){
    # produce shingles (con repeticiones)
    if(lowercase) {
      string <- str_to_lower(string)
    }
    shingles <- seq(1, nchar(string) - k + 1) %>%
        map_chr(function(x) substr(string, x, x + k - 1))
    shingles
}
minhashes <- minhash_generator(60, seed = 1223)
textos <- as.character(articulos_df$categorias)
names(textos) <- articulos_df$articulo
system.time(
wiki_corpus <-  TextReuseCorpus(
                text = textos, 
                tokenizer = tokenize_sp,
                minhash_func = minhashes,
                skip_short = FALSE)
)
```

```{r}
str(wiki_corpus[[1002]])
```

### Agrupar en cubetas {-}


```{r}
lsh_wiki <- lsh(wiki_corpus, bands = 20)
```

```{r}
lsh_wiki %>% sample_n(20)
```

Agrupamos por cubetas y filtramos las cubetas con más de un documento:

```{r}
cubetas_df <- lsh_wiki %>% 
             group_by(buckets) %>%
             summarise(candidatos = list(doc)) %>%
             mutate(num_docs = map_int(candidatos, length)) %>%
             filter(num_docs > 1)
```

```{r}
cubetas_df <- cubetas_df %>% arrange(desc(num_docs)) 
nrow(cubetas_df)
```

```{r}
sample_n(cubetas_df, 20)
```

```{r}
cubetas_df$candidatos[[1]]
lapply(cubetas_df$candidatos[[1]], function(articulo) 
  wiki_corpus[[articulo]]$content) %>% head
```

```{r}
cubetas_df$candidatos[[714]]
lapply(cubetas_df$candidatos[[714]], function(articulo) 
  wiki_corpus[[articulo]]$content) %>% head
```

```{r}
cubetas_df$candidatos[[911]]
lapply(cubetas_df$candidatos[[911]], function(articulo) 
  wiki_corpus[[articulo]]$content) %>% head
```

### Consulta de pares similares {-}

Podemos buscar rápidamente candidatos

```{r}
lsh_query(lsh_wiki, 'October_1')
```

```{r}
lsh_query(lsh_wiki, 'Icosahedron')
```

Veamos por qué esta última lista se ve así. Examinamos dos ejemplos, y
vemos que en efecto su similitud no es tan baja:

```{r}
wiki_corpus[["Icosahedron"]]
wiki_corpus[["Disaster"]]

```

```{r}
minhash_estimate <- function(a, b, corpus){
  mean(corpus[[a]]$minhashes == corpus[[b]]$minhashes)
}

lsh_query(lsh_wiki, 'Icosahedron') %>% 
  rowwise %>%
  mutate(score = minhash_estimate(a, b, wiki_corpus)) %>% 
  arrange(desc(score))

```



### Filtrar falsos positivos:

Y también podemos preprocesar todos los candidatos y eliminar
los falsos positivos:


```{r}
wiki_candidatos <- lsh_candidates(lsh_wiki)
wiki_candidatos %>% nrow
```

Tenemos que evaluar estos 30 mil resultados (antes tendríamos
que haber evaluado alrededor de 127 millones de pares). Calculamos
el score:

```{r}
wiki_candidatos <- 
  wiki_candidatos %>% 
  rowwise %>%
  mutate(score = minhash_estimate(a, b, wiki_corpus))
wiki_candidatos %>% sample_n(20)
```

```{r}
qplot(wiki_candidatos$score)
candidatos_finales <- filter(wiki_candidatos, score > 0.3)
nrow(candidatos_finales)
```

```{r}
candidatos_finales %>% 
  sample_n(200)
```

```{r, message = FALSE}
lsh_query(lsh_wiki, 'Economy_of_Paraguay') %>% 
  left_join(candidatos_finales) %>%
  filter(!is.na(score))
```


```{r, message = FALSE}
lsh_query(lsh_wiki, 'Icosahedron') %>% 
  left_join(candidatos_finales) %>%
  filter(!is.na(score))
```

```{r, message = FALSE}
lsh_query(lsh_wiki, 'Ghana') %>% 
  left_join(candidatos_finales) %>%
  filter(!is.na(score))
```
